{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled162.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYeMpL7qOdVGKi6R1b6/JL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manjotmb20/EC3/blob/master/Waveform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "462CQcLDUJXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "d=open(\"waveform.txt\").read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjbFUDltUXLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c=d.split('\\n')\n",
        "e=list()\n",
        "df=pd.DataFrame()\n",
        "for i in range(len(c)-1):\n",
        "  df[i]=c[i].split(',')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKPkUTwUUk8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSAJM00-Uqgc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "4e5a509f-a4f7-4fea-d53c-576ee0711408"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.23</td>\n",
              "      <td>-1.21</td>\n",
              "      <td>1.20</td>\n",
              "      <td>1.23</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2.49</td>\n",
              "      <td>1.19</td>\n",
              "      <td>1.34</td>\n",
              "      <td>0.58</td>\n",
              "      <td>1.22</td>\n",
              "      <td>2.30</td>\n",
              "      <td>4.65</td>\n",
              "      <td>5.82</td>\n",
              "      <td>4.91</td>\n",
              "      <td>3.16</td>\n",
              "      <td>2.25</td>\n",
              "      <td>4.01</td>\n",
              "      <td>0.82</td>\n",
              "      <td>2.28</td>\n",
              "      <td>1.01</td>\n",
              "      <td>0.09</td>\n",
              "      <td>1.22</td>\n",
              "      <td>-0.23</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>1.11</td>\n",
              "      <td>0.02</td>\n",
              "      <td>-1.32</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.47</td>\n",
              "      <td>-0.63</td>\n",
              "      <td>-0.86</td>\n",
              "      <td>-0.70</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.34</td>\n",
              "      <td>-0.13</td>\n",
              "      <td>-0.87</td>\n",
              "      <td>0.56</td>\n",
              "      <td>-0.53</td>\n",
              "      <td>0.29</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.38</td>\n",
              "      <td>0.38</td>\n",
              "      <td>-0.31</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>1.52</td>\n",
              "      <td>1.35</td>\n",
              "      <td>1.49</td>\n",
              "      <td>3.81</td>\n",
              "      <td>2.33</td>\n",
              "      <td>1.34</td>\n",
              "      <td>1.45</td>\n",
              "      <td>3.70</td>\n",
              "      <td>3.08</td>\n",
              "      <td>5.01</td>\n",
              "      <td>3.27</td>\n",
              "      <td>3.40</td>\n",
              "      <td>2.23</td>\n",
              "      <td>2.56</td>\n",
              "      <td>2.15</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>-0.37</td>\n",
              "      <td>-1.78</td>\n",
              "      <td>-0.40</td>\n",
              "      <td>0.45</td>\n",
              "      <td>-1.32</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.11</td>\n",
              "      <td>-0.78</td>\n",
              "      <td>2.36</td>\n",
              "      <td>0.31</td>\n",
              "      <td>1.28</td>\n",
              "      <td>1.40</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.91</td>\n",
              "      <td>-0.79</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.69</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.08</td>\n",
              "      <td>1.48</td>\n",
              "      <td>2.44</td>\n",
              "      <td>3.39</td>\n",
              "      <td>3.09</td>\n",
              "      <td>4.08</td>\n",
              "      <td>5.48</td>\n",
              "      <td>3.61</td>\n",
              "      <td>0.47</td>\n",
              "      <td>1.68</td>\n",
              "      <td>2.35</td>\n",
              "      <td>-1.04</td>\n",
              "      <td>0.63</td>\n",
              "      <td>-0.87</td>\n",
              "      <td>-0.80</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.35</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>-1.42</td>\n",
              "      <td>2.14</td>\n",
              "      <td>0.57</td>\n",
              "      <td>1.21</td>\n",
              "      <td>-1.64</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.41</td>\n",
              "      <td>-0.12</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.29</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.17</td>\n",
              "      <td>0.18</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>-1.33</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.27</td>\n",
              "      <td>1.39</td>\n",
              "      <td>1.03</td>\n",
              "      <td>-0.32</td>\n",
              "      <td>-1.23</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.87</td>\n",
              "      <td>1.27</td>\n",
              "      <td>4.41</td>\n",
              "      <td>3.51</td>\n",
              "      <td>4.88</td>\n",
              "      <td>4.66</td>\n",
              "      <td>5.71</td>\n",
              "      <td>2.72</td>\n",
              "      <td>0.95</td>\n",
              "      <td>1.94</td>\n",
              "      <td>0.47</td>\n",
              "      <td>-0.47</td>\n",
              "      <td>1.21</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.57</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.20</td>\n",
              "      <td>3.10</td>\n",
              "      <td>-0.34</td>\n",
              "      <td>1.18</td>\n",
              "      <td>0.43</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>-0.75</td>\n",
              "      <td>1.11</td>\n",
              "      <td>1.35</td>\n",
              "      <td>-1.63</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.81</td>\n",
              "      <td>1.59</td>\n",
              "      <td>-0.69</td>\n",
              "      <td>1.16</td>\n",
              "      <td>4.22</td>\n",
              "      <td>4.98</td>\n",
              "      <td>4.52</td>\n",
              "      <td>2.54</td>\n",
              "      <td>5.60</td>\n",
              "      <td>4.66</td>\n",
              "      <td>4.25</td>\n",
              "      <td>1.58</td>\n",
              "      <td>2.51</td>\n",
              "      <td>2.40</td>\n",
              "      <td>1.82</td>\n",
              "      <td>0.34</td>\n",
              "      <td>2.09</td>\n",
              "      <td>0.29</td>\n",
              "      <td>-2.47</td>\n",
              "      <td>2.47</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.66</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.21</td>\n",
              "      <td>1.19</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>-1.43</td>\n",
              "      <td>-0.29</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.14</td>\n",
              "      <td>1.33</td>\n",
              "      <td>-1.87</td>\n",
              "      <td>1.48</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.58</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.40</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0      1      2      3      4   ...     36     37     38    39 40\n",
              "0  -0.23  -1.21   1.20   1.23  -0.10  ...  -0.87   0.56  -0.53  0.29  2\n",
              "1   0.38   0.38  -0.31  -0.09   1.52  ...   0.04   0.91  -0.79  0.22  0\n",
              "2  -0.69   1.00   1.08   1.48   2.44  ...   0.18  -0.09  -1.33  1.00  1\n",
              "3   0.40   0.68   0.27   1.39   1.03  ...   1.11   1.35  -1.63  0.10  0\n",
              "4  -0.81   1.59  -0.69   1.16   4.22  ...  -0.02  -0.58   0.93  0.40  1\n",
              "\n",
              "[5 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvXXE2cSU-Hj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=df.iloc[:,-1]\n",
        "X=df.iloc[:,:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ3a3P2pbCh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(X, y,test_size=0.2,random_state=9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knS2tCvebzcQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "247fde3e-43f6-40da-abd3-a7e01267a821"
      },
      "source": [
        "def roc_auc_score_multiclass(actual_class, pred_class, average = \"macro\"):\n",
        "\n",
        "  #creating a set of all the unique classes using the actual class list\n",
        "  unique_class = set(actual_class)\n",
        "  roc_auc_dict = {}\n",
        "  for per_class in unique_class:\n",
        "    #creating a list of all the classes except the current class \n",
        "    other_class = [x for x in unique_class if x != per_class]\n",
        "\n",
        "    #marking the current class as 1 and all other classes as 0\n",
        "    new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
        "    new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
        "\n",
        "    #using the sklearn metrics method to calculate the roc_auc_score\n",
        "    roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
        "    roc_auc_dict[per_class] = roc_auc\n",
        "\n",
        "  return roc_auc_dict\n",
        "\n",
        "print(\"\\nLogistic Regression\")\n",
        "# assuming your already have a list of actual_class and predicted_class from the logistic regression classifier\n",
        "lr_roc_auc_multiclass = roc_auc_score_multiclass(y_val, ypred3)\n",
        "print(lr_roc_auc_multiclass)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Logistic Regression\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-42c53f0ae903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nLogistic Regression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# assuming your already have a list of actual_class and predicted_class from the logistic regression classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mlr_roc_auc_multiclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score_multiclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypred3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_roc_auc_multiclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-42c53f0ae903>\u001b[0m in \u001b[0;36mroc_auc_score_multiclass\u001b[0;34m(actual_class, pred_class, average)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#using the sklearn metrics method to calculate the roc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_actual_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_pred_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mroc_auc_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mper_class\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                              max_fpr=max_fpr),\n\u001b[1;32m    389\u001b[0m                                      \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                      sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# multilabel-indicator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         return _average_binary_score(partial(_binary_roc_auc_score,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     fpr, tpr, _ = roc_curve(y_true, y_score,\n\u001b[0;32m--> 225\u001b[0;31m                             sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \"\"\"\n\u001b[1;32m    770\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 771\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1000, 1250]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_plMT7fib3fp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "e70c409f-1c7b-44b1-d6ca-c80834cfee58"
      },
      "source": [
        "classifier=LogisticRegression(random_state=0)\n",
        "classifier.fit(x_train,y_train)\n",
        "ypred=classifier.predict(x_val)\n",
        "print(\"Accuracy score of LR\",roc_auc_score_multiclass(y_val,ypred))\n",
        "y_prob=classifier.predict_proba(x_val)\n",
        "from sklearn.svm import SVC\n",
        "classifier3=SVC(probability=True)\n",
        "df=pd.DataFrame()\n",
        "df['pred']=ypred\n",
        "survived=pd.get_dummies(df.pred,prefix='Survived')\n",
        "df=pd.concat([df,survived],axis=1)\n",
        "df.drop('pred',inplace=True,axis=1)\n",
        "df2=pd.DataFrame()\n",
        "classifier3.fit(x_train,y_train)\n",
        "ypred3=classifier3.predict(x_val)\n",
        "print(\"Accuracy score of SVM\",roc_auc_score_multiclass(y_val,ypred3))\n",
        "df2['pred']=ypred3\n",
        "y_prob3=classifier3.predict_proba(x_val)\n",
        "survived2=pd.get_dummies(df2.pred,prefix='Surviveds')\n",
        "df=pd.concat([df,survived2],axis=1)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy score of LR {'1': 0.9143639635537302, '0': 0.8877516411622687, '2': 0.9086780271751369}\n",
            "Accuracy score of SVM {'1': 0.9139764701761454, '0': 0.8812472386970667, '2': 0.90434276705953}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_yEoTGFb-AG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "05a4e3c5-a246-4ed5-a74a-bea63cf1ce5d"
      },
      "source": [
        "dt=DecisionTreeClassifier()\n",
        "dt.fit(x_train,y_train)\n",
        "d_pred=dt.predict(x_val)\n",
        "d_prob=dt.predict_proba(x_val)\n",
        "accuracy_score(y_val,d_pred)\n",
        "df_dt=pd.DataFrame()\n",
        "df_dt['Survived_k']=d_pred\n",
        "survivedt=pd.get_dummies(df_dt.Survived_k,prefix='Survivedk')\n",
        "df['Survivedt_0']=survivedt['Survivedk_0']\n",
        "df['Survivedt_1']=survivedt['Survivedk_1']\n",
        "print(\"Accuracy score of DT\",roc_auc_score_multiclass(y_val,d_pred))\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score of DT {'1': 0.8191982170926175, '0': 0.792180436363961, '2': 0.8027644320996923}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-qCj6ureD76",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "054a78e2-b283-4f30-8c09-d8774dd30fd2"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "gb=GaussianNB()\n",
        "gb.fit(x_train,y_train)\n",
        "g_pred=gb.predict(x_val)\n",
        "g_prob=gb.predict_proba(x_val)\n",
        "accuracy_score(y_val,g_pred)\n",
        "df_gb=pd.DataFrame()\n",
        "df_gb['Survived_k']=g_pred\n",
        "survivedg=pd.get_dummies(df_gb.Survived_k,prefix='Survivedg')\n",
        "df['Survivedg_0']=survivedg['Survivedg_0']\n",
        "df['Survivedg_1']=survivedg['Survivedg_1']\n",
        "print(\"Accuracy score of DT\",roc_auc_score_multiclass(y_val,g_pred))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score of DT {'1': 0.9008323445319648, '0': 0.7617804435042999, '2': 0.9015839651677802}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkSgJoYHd1mO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "7b4aa4d3-a2e3-424c-f38d-43af6f26a7c0"
      },
      "source": [
        "import xgboost as xgb\n",
        "xgb=xgb.XGBClassifier()\n",
        "xgb.fit(x_train,y_train)\n",
        "x_pred=xgb.predict(x_val)\n",
        "x_prob=xgb.predict_proba(x_val)\n",
        "roc_auc_score_multiclass(y_val,x_pred)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-5bde403c6f58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             train_dmatrix = DMatrix(X, label=training_labels,\n\u001b[0;32m--> 726\u001b[0;31m                                     missing=self.missing, nthread=self.n_jobs)\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         self._Booster = train(xgb_options, train_dmatrix, self.get_num_boosting_rounds(),\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    378\u001b[0m         data, feature_names, feature_types = _maybe_pandas_data(data,\n\u001b[1;32m    379\u001b[0m                                                                 \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                                                                 feature_types)\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         data, feature_names, feature_types = _maybe_dt_data(data,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_maybe_pandas_data\u001b[0;34m(data, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    237\u001b[0m         msg = \"\"\"DataFrame.dtypes for data must be int, float or bool.\n\u001b[1;32m    238\u001b[0m                 Did not expect the data types in fields \"\"\"\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, int found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBXk2hwAe33J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eeb8877e-1d42-436e-c5b9-83b0fbf74b04"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "forest = RandomForestClassifier()\n",
        "forest.fit(x_train, y_train)\n",
        "Y_pred = forest.predict(x_val)\n",
        "print(\"Accuracy score of Random Forest\",roc_auc_score_multiclass(y_val,Y_pred))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score of Random Forest {'1': 0.9089543808643948, '0': 0.8834785946027964, '2': 0.8982058404023722}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp-aTInsfD1G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d87b12e4-48b4-4412-ea73-bc3b4853d476"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(x_train, y_train)\n",
        "Y_pred2 = knn.predict(x_val)\n",
        "print(\"Accuracy score of Kneighbors\",roc_auc_score_multiclass(y_val,Y_pred2))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score of Kneighbors {'1': 0.878948820224965, '0': 0.8329138384230561, '2': 0.8707773440432399}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmnkZaMwfpLk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a55e80f-f5d2-4a7a-a255-4a6d66e3e443"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "adb=AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=100)\n",
        "adb.fit(x_train,y_train)\n",
        "a_pred=adb.predict(x_val)\n",
        "print(\"Auc score\", roc_auc_score_multiclass(y_val,a_pred))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Auc score {'1': 0.8949192393745813, '0': 0.865891047353835, '2': 0.8810806245777343}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH8SO53QU4GK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m=np.array(df)\n",
        "s=np.zeros(shape=(m.shape[0],m.shape[0]))\n",
        "for i in range(m.shape[0]):\n",
        "  for j in range(m.shape[0]):\n",
        "    if i!=j:\n",
        "      sum1=0\n",
        "      for k in range(2,m.shape[1]):\n",
        "        sum1=sum1+(m[i][k]*m[j][k])\n",
        "      s[i][j]=sum1\n",
        "w=s\n",
        "d=np.zeros(shape=w.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG-izjB6bIYS",
        "colab_type": "code",
        "outputId": "5ea3922d-1e72-4d2d-e55e-5468423c84a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "for i in range(d.shape[0]):\n",
        "  d[i]=np.sum(w[i],axis=0)\n",
        "wnew=w/d.reshape(d.shape[0],1)\n",
        "Km=wnew\n",
        "Km\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.00272975, ..., 0.00272975, 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.00272975, 0.        , 0.        , ..., 0.00272975, 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.00272975, 0.        , 0.00272975, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.00306748],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.00306748,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y5Os1YObN9k",
        "colab_type": "code",
        "outputId": "a8bf1d42-f834-4ad2-aa10-6b19e18aa43a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(np.sum(Km,axis=1))\n",
        "print(np.sum(Km,axis=0))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "[1.03681726 0.63457318 1.03681726 1.03681726 0.90911809 1.21229922\n",
            " 0.90911809 1.12311588 1.03681726 0.82180001 0.72521986 0.90911809\n",
            " 1.03593588 1.03681726 1.03593588 1.03681726 1.03593588 1.03593588\n",
            " 0.90911809 1.21229922 1.03681726 1.21229922 1.21229922 1.22085668\n",
            " 1.31005394 1.21229922 1.03681726 1.03593588 1.03681726 1.1259961\n",
            " 1.21229922 1.03593588 1.03593588 0.94670085 0.90911809 0.82180001\n",
            " 0.90911809 1.21229922 0.90911809 1.03593588 0.90911809 1.03593588\n",
            " 1.21229922 0.90911809 1.22085668 1.03681726 0.90911809 1.03681726\n",
            " 0.90911809 1.1259961  1.22085668 0.94670085 1.1259961  0.77310714\n",
            " 1.03681726 0.90911809 0.82180001 1.31005394 0.42076959 1.12311588\n",
            " 1.03681726 1.22085668 0.63457318 1.21229922 0.90911809 1.03681726\n",
            " 0.90911809 1.03593588 0.90911809 1.03593588 1.03593588 1.03681726\n",
            " 0.82180001 1.03681726 0.90911809 0.82180001 0.90911809 1.03593588\n",
            " 1.03681726 1.03593588 1.1259961  1.31005394 1.03593588 1.03681726\n",
            " 1.03681726 0.90911809 1.22085668 1.03593588 0.90911809 0.90911809\n",
            " 1.03681726 1.03593588 0.90911809 1.03681726 1.03681726 1.03593588\n",
            " 1.31005394 1.21229922 1.12311588 1.31005394 0.90911809 0.90911809\n",
            " 1.31005394 1.1259961  0.78233311 1.03681726 0.82180001 0.82180001\n",
            " 0.90911809 0.90911809 1.03681726 0.72521986 0.90911809 1.12311588\n",
            " 0.90911809 0.90911809 1.03681726 0.90911809 1.03593588 1.31005394\n",
            " 1.03681726 0.90911809 1.03681726 1.03593588 1.22085668 1.03681726\n",
            " 1.1259961  1.03681726 1.03681726 1.03681726 0.90911809 1.03593588\n",
            " 1.03593588 1.03681726 1.03681726 1.03593588 0.72521986 1.03681726\n",
            " 1.31005394 1.03681726 1.03681726 1.21229922 1.03593588 0.90911809\n",
            " 0.90911809 0.90911809 0.72521986 1.03681726 1.31005394 1.03681726\n",
            " 1.03593588 1.31005394 1.03681726 1.03681726 0.72521986 1.03681726\n",
            " 1.03681726 0.90911809 1.03681726 1.03681726 0.42076959 1.03593588\n",
            " 0.90911809 0.42076959 1.03681726 0.90911809 0.64631988 1.03681726\n",
            " 1.03593588 1.1259961  0.73255556 0.82180001 1.03681726 0.90911809\n",
            " 1.21229922 1.1259961  1.03593588 1.03593588 1.03681726 1.03593588\n",
            " 1.21229922 1.03681726 1.03681726 1.1259961  1.03681726 0.90911809\n",
            " 1.21229922 0.90911809 1.03681726 0.90911809 1.03681726 1.03681726\n",
            " 0.82180001 1.03593588 1.12311588 1.03593588 1.03681726 1.31005394\n",
            " 0.90911809 1.03681726 0.94670085 1.31005394 1.03593588 1.03681726\n",
            " 0.90911809 1.12311588 0.90911809 0.64631988 1.22085668 1.03593588\n",
            " 1.03593588 1.03681726 0.42076959 1.31005394 1.31005394 1.03681726\n",
            " 1.03681726 0.90911809 1.03681726 1.03681726 0.90911809 0.86234705\n",
            " 1.22085668 1.03593588 1.31005394 1.03593588 0.90911809 1.12311588\n",
            " 0.90911809 0.90911809 1.03681726 1.03681726 1.03681726 1.03681726\n",
            " 0.90911809 1.03681726 1.03593588 0.72521986 1.12311588 0.90911809\n",
            " 0.42076959 1.03681726 0.72521986 0.90911809 1.03681726 0.72521986\n",
            " 1.1259961  0.82180001 0.90911809 0.82180001 1.03681726 0.90911809\n",
            " 1.03681726 1.48450519 1.03681726 1.03681726 1.22085668 1.03593588\n",
            " 1.03681726 1.31005394 1.31005394 0.72521986 0.90911809 0.90911809\n",
            " 1.31005394 0.46137494 0.90911809 1.03681726 1.03593588 1.03681726\n",
            " 1.03681726 1.03593588 0.90911809 0.90911809 1.03681726 0.90911809\n",
            " 1.03593588 0.90911809 1.03681726 0.82180001 0.42076959 1.03681726\n",
            " 0.90911809 0.90911809 1.03681726 1.03593588 0.90911809 0.90911809\n",
            " 1.1259961  0.90911809 1.03681726 1.03681726 0.90911809 0.94670085\n",
            " 0.90911809 1.03681726 1.03681726 1.1259961  1.03593588 1.09403981\n",
            " 1.03593588 0.90911809 1.31005394 1.03593588 1.61134373 1.03593588\n",
            " 0.90911809 0.90911809 1.03593588 1.22085668 1.03681726 1.03593588\n",
            " 0.72521986 1.03681726 1.03681726 1.03681726 1.03593588 1.03593588\n",
            " 1.03681726 0.82180001 0.42076959 0.90911809 0.90911809 0.90911809\n",
            " 0.82180001 1.03681726 0.90911809 1.03681726 1.21229922 1.31005394\n",
            " 1.03681726 1.03593588 1.21229922 1.03593588 1.31005394 1.03681726\n",
            " 1.03593588 0.42076959 1.03681726 0.90911809 1.03593588 0.90911809\n",
            " 0.63457318 0.90911809 1.03681726 0.90911809 1.03681726 0.82180001\n",
            " 1.03593588 1.03593588 1.21229922 0.90911809 0.86856524 1.61134373\n",
            " 1.03681726 0.90911809 1.03593588 0.90911809 1.21229922 1.1259961\n",
            " 1.03681726 1.03681726 0.72521986 0.82180001 1.03593588 0.90911809\n",
            " 1.03681726 1.03681726 0.90911809 1.1259961  1.1259961  0.90911809\n",
            " 1.22085668 1.03681726 1.31005394 0.90911809 0.90911809 1.03681726\n",
            " 0.90911809 1.1259961  1.21229922 1.22085668 1.03593588 0.82180001\n",
            " 1.03593588 1.03681726 0.90911809 0.90911809 0.90911809 0.90911809\n",
            " 1.03681726 1.21229922 1.03681726 1.21229922 0.90911809 0.90911809\n",
            " 1.03593588 1.31005394 0.72521986 1.03593588 0.90911809 0.90911809\n",
            " 1.03681726 1.31005394 0.90911809 1.03681726 1.03681726 1.03681726\n",
            " 0.90911809 1.03593588 0.90911809 1.31005394 1.1259961  0.90911809\n",
            " 1.31005394 0.90911809 1.03681726 1.31005394 1.03681726 0.46137494\n",
            " 0.78233311 1.03681726 1.1259961  0.90911809 1.03681726 1.03681726\n",
            " 0.90911809 1.1259961  1.03593588 0.90911809 0.72521986 0.90911809\n",
            " 0.42076959 1.31005394 1.03681726 0.90911809 0.82180001 0.90911809\n",
            " 0.90911809 1.03681726 1.03593588 1.1259961  1.29967211 0.90911809\n",
            " 1.21229922 0.90911809 0.90911809 0.94670085 1.31005394 1.03681726\n",
            " 0.42076959 0.90911809 1.03681726 1.12311588 0.90911809 1.03681726\n",
            " 1.03681726 1.03593588 1.03593588 1.03593588 0.90911809 1.21229922\n",
            " 1.21229922 1.03593588 1.03681726 0.94670085 0.90911809 0.90911809\n",
            " 1.03681726 1.03593588 1.22085668 1.31005394 0.42076959 1.03681726\n",
            " 1.03681726 0.90911809 0.90911809 1.1259961  1.03681726 1.1259961\n",
            " 0.90911809 0.90911809 1.03681726 0.90911809 1.03593588 1.12311588\n",
            " 0.90911809 1.03593588 1.03593588 0.90911809 0.90911809 1.03681726\n",
            " 0.63457318 1.03681726 1.03593588 1.03681726 1.03681726 1.31005394\n",
            " 1.03593588 0.90911809 1.03593588 0.94670085 0.72521986 1.03681726\n",
            " 0.42076959 1.1259961  0.90911809 1.03593588 0.42076959 1.03593588\n",
            " 1.03593588 1.1259961  0.90911809 1.03681726 1.12311588 0.90911809\n",
            " 1.03593588 1.03593588 0.90911809 0.72521986 1.21229922 0.72521986\n",
            " 1.1259961  1.03681726 0.90911809 1.03593588 1.03681726 1.03681726\n",
            " 0.90911809 0.90911809 1.03681726 0.90911809 0.90911809 1.03681726\n",
            " 1.03593588 0.90911809 1.03681726 0.72521986 0.82180001 1.03681726\n",
            " 1.03681726 1.03681726 1.03593588 1.03681726 1.03681726 1.21229922\n",
            " 1.03593588 1.03681726 1.03681726 1.12311588 1.03593588 1.03681726\n",
            " 0.90911809 0.82180001 1.1259961  1.03681726 1.03593588 1.21229922\n",
            " 1.1259961  1.03593588 1.03681726 0.90911809 1.31005394 0.90911809\n",
            " 1.03681726 1.21229922 1.21229922 1.03681726 0.46137494 1.03681726\n",
            " 1.12311588 0.90911809 1.26952128 0.90911809 0.90911809 0.82180001\n",
            " 1.1259961  1.03593588 0.90911809 0.90911809 0.90911809 1.03681726\n",
            " 0.90911809 0.86234705 0.82180001 0.90911809 1.03681726 1.03593588\n",
            " 1.03593588 1.03681726 1.03593588 1.03593588 1.03681726 1.03593588\n",
            " 1.12311588 0.72521986 1.03681726 0.90911809 1.03681726 0.90911809\n",
            " 0.82180001 1.12311588 1.03681726 1.03593588 0.90911809 1.31005394\n",
            " 0.90911809 1.03681726 0.90911809 0.94670085 0.90911809 1.03681726\n",
            " 0.90911809 1.12311588 1.03593588 0.90911809 1.03593588 1.21229922\n",
            " 0.33467661 1.03593588 1.03681726 0.90911809 1.22085668 1.03593588\n",
            " 0.94968111 1.03593588 1.03593588 0.90911809 1.03681726 1.03681726\n",
            " 0.72521986 0.72521986 0.63457318 0.90911809 1.31005394 1.03593588\n",
            " 1.03681726 1.29967211 1.31005394 0.90911809 1.03593588 0.90911809\n",
            " 1.03593588 1.03593588 0.82180001 1.03593588 1.03681726 1.1259961\n",
            " 1.03593588 1.61134373 1.03593588 1.1259961  1.03681726 0.90911809\n",
            " 1.21229922 0.90911809 1.22085668 1.03593588 0.42076959 1.22085668\n",
            " 0.46137494 1.03681726 1.03681726 1.31005394 0.82180001 1.03681726\n",
            " 0.72521986 0.90911809 0.90911809 0.90911809 1.03593588 1.03681726\n",
            " 1.21229922 0.82180001 1.12311588 1.03681726 1.31005394 1.03593588\n",
            " 1.03681726 0.90911809 1.61134373 1.03593588 1.31005394 1.03681726\n",
            " 1.03681726 1.03593588 1.03681726 1.03681726 1.03681726 1.03681726\n",
            " 1.03681726 1.03593588 1.03681726 0.86234705 1.03681726 0.90911809\n",
            " 1.03681726 0.90911809 1.03593588 1.31005394 1.21229922 1.21229922\n",
            " 1.03681726 0.86234705 1.31005394 0.64631988 1.03593588 1.03681726\n",
            " 1.1259961  0.90911809 1.03593588 1.03593588 1.03681726 1.03681726\n",
            " 1.21229922 1.03593588 1.03681726 0.90911809 1.22085668 0.90911809\n",
            " 0.90911809 1.03681726 1.03593588 1.03681726 0.42076959 1.21229922\n",
            " 0.90911809 1.03593588 1.03681726 1.03681726 1.03593588 0.90911809\n",
            " 1.1259961  1.03681726 1.03681726 1.31005394 0.72521986 1.03593588\n",
            " 1.1259961  1.03681726 1.03681726 0.73255556 1.03681726 0.82180001\n",
            " 0.90911809 1.21229922 1.03681726 1.31005394 1.1259961  1.21229922\n",
            " 0.82180001 0.90911809 1.21229922 0.90911809 1.03681726 1.03593588\n",
            " 0.90911809 0.82180001 1.21229922 0.33467661 0.42076959 0.42076959\n",
            " 1.03593588 0.90911809 0.94670085 1.03681726 1.03681726 1.03681726\n",
            " 1.1259961  1.03681726 1.03593588 1.31005394 1.03681726 1.03593588\n",
            " 1.03681726 0.90911809 1.21229922 1.03681726 1.21229922 0.82180001\n",
            " 1.03681726 1.03681726 1.03681726 1.03593588 1.22085668 0.90911809\n",
            " 0.90911809 1.03681726 0.82180001 1.22085668 0.82180001 1.03593588\n",
            " 0.82180001 1.1259961  1.03681726 0.90911809 1.03681726 1.22085668\n",
            " 1.12311588 1.03681726 1.21229922 0.90911809 0.90911809 1.03681726\n",
            " 0.90911809 1.03681726 1.03681726 0.90911809 1.52434802 1.03593588\n",
            " 0.90911809 0.63457318 1.03681726 0.90911809 1.03681726 0.90911809\n",
            " 1.03593588 1.03681726 0.72521986 1.31005394 1.03681726 1.1259961\n",
            " 1.22085668 1.03681726 1.03681726 1.1259961  0.90911809 1.03681726\n",
            " 0.90911809 1.03593588 1.31005394 1.03593588 0.94670085 0.90911809\n",
            " 0.82180001 0.90911809 0.90911809 1.03593588 1.03681726 1.03681726\n",
            " 0.90911809 1.03681726 0.90911809 1.21229922 0.63457318 1.03681726\n",
            " 0.72521986 0.90911809 1.03681726 1.21229922 0.42076959 1.03593588\n",
            " 0.82180001 1.03593588 1.03593588 1.1259961  0.72521986 1.03681726\n",
            " 1.03681726 1.1259961  0.90911809 1.22085668 1.03681726 1.03593588\n",
            " 1.1259961  1.21229922 1.31005394 0.90911809 1.03593588 1.03593588\n",
            " 1.1259961  1.03681726 1.03593588 1.31005394 1.1259961  0.90911809\n",
            " 0.72521986 1.03681726 1.03593588 1.1259961  0.90911809 0.86234705\n",
            " 0.42076959 1.03681726 1.03681726 1.03681726 1.03593588 1.03681726\n",
            " 1.21229922 1.03593588 0.90911809 0.90911809 0.90911809 1.21229922\n",
            " 1.03681726 1.03681726 1.03593588 0.86234705 1.03681726 1.1259961\n",
            " 1.22085668 1.03593588 1.03681726 0.82180001 1.21229922 1.31005394\n",
            " 0.90911809 0.42076959 1.03681726 0.90911809 1.1259961  0.42076959\n",
            " 0.90911809 0.42076959 0.82180001 1.03593588 1.03681726 1.03681726\n",
            " 0.90911809 1.03593588 1.31005394 0.90911809 0.90911809 0.90911809\n",
            " 0.90911809 0.90911809 1.21229922 1.31005394 1.03593588 1.03593588\n",
            " 1.03593588 0.90911809 0.42076959 1.03681726 1.03681726 1.03593588\n",
            " 0.90911809 0.90911809 1.03593588 0.90911809 1.03593588 1.22085668\n",
            " 1.03681726 0.90911809 1.03681726 0.90911809 0.82180001 0.90911809\n",
            " 1.1259961  0.90911809 0.82180001 0.33467661 1.03681726 0.90911809\n",
            " 0.90911809 1.21229922 1.03681726 0.82180001 1.03681726 1.03681726\n",
            " 0.90911809 1.03681726 0.90911809 1.03681726 1.29967211 0.82180001\n",
            " 1.03593588 1.12311588 1.03681726 0.90911809 1.03681726 1.1259961\n",
            " 0.72521986 0.77310714 0.82180001 1.21229922 0.90911809 1.03681726\n",
            " 0.42076959 1.03681726 1.03593588 1.03681726 0.94670085 0.90911809\n",
            " 1.03681726 1.03593588 1.03593588 0.90911809 1.31005394 1.31005394\n",
            " 0.90911809 1.03681726 1.31005394 0.42076959 1.03681726 1.03681726\n",
            " 0.90911809 1.03681726 0.90911809 0.90911809]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1V-98pImkVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "m=m.dot(m.T)  \n",
        "s=np.zeros(shape=(m.shape[0],m.shape[0]))\n",
        "for i in range(m.shape[0]):\n",
        "  for j in range(m.shape[0]):\n",
        "    if i!=j:\n",
        "      sum1=0\n",
        "      for k in range(2,m.shape[1]):\n",
        "        sum1=sum1+(m[i][k]*m[j][k])\n",
        "      s[i][j]=sum1   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j40zOKQ-mnOz",
        "colab_type": "code",
        "outputId": "27f75eca-bfdf-42cd-f186-ad491b8d6f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "d=np.zeros(shape=w.shape[0])\n",
        "w=s\n",
        "for i in range(d.shape[0]):\n",
        "  d[i]=np.sum(w[i],axis=0)\n",
        "wnew=w/d.reshape(d.shape[0],1)  \n",
        "Kc=wnew\n",
        "Kc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.00159626, 0.00159626, ..., 0.00013716, 0.00159626,\n",
              "        0.00086233],\n",
              "       [0.00159626, 0.        , 0.00159626, ..., 0.00013716, 0.00159626,\n",
              "        0.00086233],\n",
              "       [0.00159626, 0.00159626, 0.        , ..., 0.00013716, 0.00159626,\n",
              "        0.00086233],\n",
              "       ...,\n",
              "       [0.00032536, 0.00032536, 0.00032536, ..., 0.        , 0.00032536,\n",
              "        0.00125644],\n",
              "       [0.00159626, 0.00159626, 0.00159626, ..., 0.00013716, 0.        ,\n",
              "        0.00086233],\n",
              "       [0.00122112, 0.00122112, 0.00122112, ..., 0.00075003, 0.00122112,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAFnKTDscdnX",
        "colab_type": "text"
      },
      "source": [
        "**Same, checking for bi-stochastic behaviour**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlvTfnrF2Smi",
        "colab_type": "code",
        "outputId": "d6fc24bb-a420-4e80-8a80-13506afbe222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "source": [
        "  np.sum(Kc,axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmJ1FML0ckLi",
        "colab_type": "text"
      },
      "source": [
        "**Generating Fg and Fo i'e the Group class matrix and Object Class matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTO5d9Ay2Xc5",
        "colab_type": "code",
        "outputId": "1a210a8c-74f2-4e55-fd83-3eef07c881e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "\n",
        "objclass_mat1=pd.DataFrame(y_prob,columns=['Survived','NotSurvived'])\n",
        "objclass_mat2=pd.DataFrame(y_prob3,columns=['Survived','NotSurvived'])\n",
        "df_t=pd.DataFrame(objclass_mat1)\n",
        "Fo=np.array(df_t)\n",
        "gpclass_mat=pd.DataFrame(columns=['Survived','NotSurvived'],index=['class1surv','class1notsurv','class2surv','class2notsurv'])\n",
        "prob1s=np.sum(objclass_mat1.Survived)/len(objclass_mat1)\n",
        "prob1ns=np.sum(objclass_mat1.NotSurvived)/len(objclass_mat1)\n",
        "prob2s=np.sum(objclass_mat2.Survived)/len(objclass_mat2)\n",
        "prob2ns=np.sum(objclass_mat2.NotSurvived)/len(objclass_mat2)\n",
        "list1=[]\n",
        "list1.append(prob1s)\n",
        "list1.append(prob1ns)\n",
        "list1.append(prob2s)\n",
        "list1.append(prob2ns)\n",
        "\n",
        "list2=[]\n",
        "list2.append(prob1ns)\n",
        "list2.append(prob1s)\n",
        "list2.append(prob2ns)\n",
        "list2.append(prob2s)\n",
        "gpclass_mat['Survived']=list1\n",
        "gpclass_mat['NotSurvived']=list2\n",
        "gpclass_mat\n",
        "Fg=np.array(gpclass_mat)\n",
        "Fg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.38290849, 0.61709151],\n",
              "       [0.61709151, 0.38290849],\n",
              "       [0.37733888, 0.62266112],\n",
              "       [0.62266112, 0.37733888]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRxO_k8tct-4",
        "colab_type": "text"
      },
      "source": [
        "**Similarly, generating  Yo and Yg**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovw7WJ7S2pQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n=ypred.shape[1]\n",
        "C=survived.shape[1]\n",
        "G=survived.shape[1]*n\n",
        "\n",
        "Yo=np.zeros(shape=(df.shape[0],C))\n",
        "cal_y=np.array(df)\n",
        "for i in range(Yo.shape[0]):\n",
        "  Yo[i][0]=(cal_y[i][0]+cal_y[i][2])/C\n",
        "for i in range(Yo.shape[0]):\n",
        "  Yo[i][1]=(cal_y[i][1]+cal_y[i][3])/C"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SR3KDg3q87U",
        "colab_type": "code",
        "outputId": "bf49d056-90f0-4fc1-b9f1-4756678c2ffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "G"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYZkiSfs2w2I",
        "colab_type": "code",
        "outputId": "e7b32840-a7ac-41ea-881a-76cf6c45d19a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "m=np.array(df)\n",
        "Yg=np.zeros(shape=(m.shape[1],2))\n",
        "c=np.sum(m,axis=0)\n",
        "for i in range(Yg.shape[0]):\n",
        "  if i%2==0:\n",
        "    Yg[i][0]=c[i]/m.shape[0]\n",
        "    Yg[i][1]=1-Yg[i][0]\n",
        "  else:\n",
        "    Yg[i][1]=c[i]/m.shape[0]\n",
        "    Yg[i][0]=1-Yg[i][1]  \n",
        "Yg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.38357588, 0.61642412],\n",
              "       [0.38357588, 0.61642412],\n",
              "       [0.37733888, 0.62266112],\n",
              "       [0.37733888, 0.62266112]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTa6_2DsdIlO",
        "colab_type": "text"
      },
      "source": [
        "Genearting "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hcY4DpgduI4",
        "colab_type": "text"
      },
      "source": [
        "Checking for the conditions specified in eq 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHAljZHU21jH",
        "colab_type": "code",
        "outputId": "540d7e59-f2da-4e4c-b3eb-3ce5d4560b5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "Fo_cond=True\n",
        "Fg_cond=True\n",
        "for i in range(Fo.shape[0]):\n",
        "  if np.linalg.norm((Fo[i]), ord=1)!=1:\n",
        "    Fo_cond=False\n",
        "for i in range(Fg.shape[0]):\n",
        "  if np.linalg.norm((Fg[i]), ord=1)!=1:\n",
        "    Fo_cond=False    \n",
        "print(Fo_cond)\n",
        "print(Fg_cond)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7YlgtdbeCFc",
        "colab_type": "text"
      },
      "source": [
        "**Generating diagonal matrices Dm and Dc**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8cG1naJ3Cbk",
        "colab_type": "code",
        "outputId": "de8051a5-7be7-4f8f-80aa-86311cad56c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_dm=np.zeros(shape=(Km.shape))\n",
        "test_Km=Km\n",
        "for i in range(test_Km.shape[0]):\n",
        "  test_dm[i]=np.sum(test_Km[i],axis=0)\n",
        "Dm=np.diag(np.diag(test_dm))\n",
        "print(\"Shape of DM\",Dm.shape)\n",
        "test_dc=np.zeros(shape=(Kc.shape))\n",
        "test_Kc=Kc\n",
        "for i in range(test_Kc.shape[0]):\n",
        "  test_dc[i]=np.sum(test_Kc[i],axis=0)\n",
        "Dc=np.diag(np.diag(test_dc))\n",
        "print(\"shape of Dc\",Dc.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of DM (962, 962)\n",
            "shape of Dc (962, 962)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv1sYezy3Hwa",
        "colab_type": "code",
        "outputId": "833c8ad4-4d56-4534-fd3d-dceb05198e21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(\"KHM\",Km.shape)\n",
        "print(\"KHC\",Kc.shape)\n",
        "print(\"Fo\",Fo.shape)\n",
        "print(\"Fg\",Fg.shape)\n",
        "print(\"Yo\",Yo.shape)\n",
        "print(\"Yg\",Yg.shape)\n",
        "print(\"Dm\",Dm.shape)\n",
        "print(\"Dc\",Dc.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KHM (962, 962)\n",
            "KHC (962, 962)\n",
            "Fo (962, 2)\n",
            "Fg (4, 2)\n",
            "Yo (962, 2)\n",
            "Yg (4, 2)\n",
            "Dm (962, 962)\n",
            "Dc (962, 962)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCBI0yrwqzOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5InaYBP3Lfq",
        "colab_type": "code",
        "outputId": "dec2403b-52ab-4248-a419-32f542148acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "Yg=np.pad(Yg,pad_width=[(0, Km.shape[0]-G), (0, 0)])\n",
        "Yg.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(962, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro1G66qS5Pzv",
        "colab_type": "code",
        "outputId": "4c79c4aa-e960-4bf8-97c8-029ca5769ee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(\"KHM\",Km.shape)\n",
        "print(\"KHC\",Kc.shape)\n",
        "print(\"Fo\",Fo.shape)\n",
        "print(\"Fg\",Fg.shape)\n",
        "print(\"Yo\",Yo.shape)\n",
        "print(\"Yg\",Yg.shape)\n",
        "print(\"Dm\",Dm.shape)\n",
        "print(\"Dc\",Dc.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KHM (962, 962)\n",
            "KHC (962, 962)\n",
            "Fo (962, 2)\n",
            "Fg (4, 2)\n",
            "Yo (962, 2)\n",
            "Yg (962, 2)\n",
            "Dm (962, 962)\n",
            "Dc (962, 962)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OVQOQf35Tb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "threshold=sys.float_info.epsilon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2yQviy9gmJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import linalg as LA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTILUgDfeX91",
        "colab_type": "text"
      },
      "source": [
        "**Implementing algorithm 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaOULbFt5XBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha=0.25\n",
        "beta=0.35\n",
        "gamma=0.35\n",
        "delta=0.05\n",
        "Fo_t=np.zeros(shape=Fo.shape)\n",
        "Fg_t=np.zeros(shape=Fg.shape)\n",
        "Fot_old=np.zeros(shape=Fo.shape)\n",
        "Fo_t=Fo\n",
        "Fg_t=Fg\n",
        "while LA.norm((Fo_t-Fot_old),'fro')>threshold:\n",
        "  Fot_old=Fo_t\n",
        "  Fg_t=np.matmul((np.linalg.inv(2*delta*np.ones(shape=Dm.shape)+(alpha*Dm))),((alpha*np.matmul(Km,Fo_t))+(2*delta*Yg)))\n",
        "  Fo_t=np.matmul((np.linalg.inv((alpha*Dm)+(2*beta*Dc)-(beta*np.matmul(np.identity(Kc.shape[0]),Kc))-(beta*np.matmul(np.ones(shape=Kc.shape),Kc))+(2*gamma*np.identity(Kc.shape[0])))),((alpha*np.matmul(Km,Fg_t))+(2*gamma*Yo)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhH4nVo_5e6_",
        "colab_type": "code",
        "outputId": "70393647-d182-4c09-e066-699bae22147d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score,roc_auc_score\n",
        "result=np.zeros(shape=Fo_t.shape[0])\n",
        "for i in range(Fo_t.shape[0]):\n",
        "  if Fo_t[i][0]<Fo_t[i][1]:\n",
        "    result[i]=1\n",
        "print(\"Accuracy value for Ensemble model\",accuracy_score(y_val,result))\n",
        "print(\"AUC for for Ensemble model\",roc_auc_score(y_val,result))\n",
        "print(\"Accuracy value for LR\",accuracy_score(y_val,ypred))\n",
        "print(\"AUC for for LR\",roc_auc_score(y_val,ypred))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy value for Ensemble model 0.8835758835758836\n",
            "AUC for for Ensemble model 0.8977652312831311\n",
            "Accuracy value for LR 0.8056133056133056\n",
            "AUC for for LR 0.7942188294241523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUskNBtfe5C7",
        "colab_type": "text"
      },
      "source": [
        "**We see that the ensemble model performs better than the Random forest, Kneigbours and Svm models and is at par with the Logistic regression model that gives us the best accuracy. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-wv5VdjiJm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}