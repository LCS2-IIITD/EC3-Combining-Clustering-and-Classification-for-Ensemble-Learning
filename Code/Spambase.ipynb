{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spambase.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manjotmb20/EC3/blob/master/Spambase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuTaqUS87Pvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from numpy import linalg as LA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GRNPI5wkDiZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "890426c0-240c-4f66-848b-fd5b41bd765a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Styles\n",
        "plt.style.use('ggplot')\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "plt.rcParams['font.family'] = 'serif'\n",
        "plt.rcParams['font.serif'] = 'Ubuntu'\n",
        "plt.rcParams['font.monospace'] = 'Ubuntu Mono'\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.labelsize'] = 10\n",
        "plt.rcParams['xtick.labelsize'] = 8\n",
        "plt.rcParams['ytick.labelsize'] = 8\n",
        "plt.rcParams['legend.fontsize'] = 10\n",
        "plt.rcParams['figure.titlesize'] = 12\n",
        "plt.rcParams['patch.force_edgecolor'] = True\n",
        "\n",
        "# Text Preprocessing\n",
        "import nltk\n",
        "nltk.download(\"all\")\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKMrRVNIkcAt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "outputId": "447815ab-6685-416d-dade-608c9dd34a7b"
      },
      "source": [
        "messages = pd.read_csv(\"spam.csv\", encoding = 'latin-1')\n",
        "\n",
        "# Drop the extra columns and rename columns\n",
        "\n",
        "messages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
        "messages.columns = [\"category\", \"text\"]\n",
        "topMessages = messages.groupby(\"text\")[\"category\"].agg([len, np.max]).sort_values(by = \"len\", ascending = False).head(n = 10)\n",
        "display(topMessages)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>len</th>\n",
              "      <th>amax</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Sorry, I'll call later</th>\n",
              "      <td>30</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I cant pick the phone right now. Pls send a message</th>\n",
              "      <td>12</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ok...</th>\n",
              "      <td>10</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Your opinion about me? 1. Over 2. Jada 3. Kusruthi 4. Lovable 5. Silent 6. Spl character 7. Not matured 8. Stylish 9. Simple Pls reply..</th>\n",
              "      <td>4</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Wen ur lovable bcums angry wid u, dnt take it seriously.. Coz being angry is d most childish n true way of showing deep affection, care n luv!.. kettoda manda... Have nice day da.</th>\n",
              "      <td>4</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Please call our customer service representative on FREEPHONE 0808 145 4742 between 9am-11pm as you have WON a guaranteed å£1000 cash or å£5000 prize!</th>\n",
              "      <td>4</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Okie</th>\n",
              "      <td>4</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Say this slowly.? GOD,I LOVE YOU &amp;amp; I NEED YOU,CLEAN MY HEART WITH YOUR BLOOD.Send this to Ten special people &amp;amp; u c miracle tomorrow, do it,pls,pls do it...</th>\n",
              "      <td>4</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7 wonders in My WORLD 7th You 6th Ur style 5th Ur smile 4th Ur Personality 3rd Ur Nature 2nd Ur SMS and 1st \\Ur Lovely Friendship\\\"... good morning dear\"</th>\n",
              "      <td>4</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ok.</th>\n",
              "      <td>4</td>\n",
              "      <td>ham</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    len  amax\n",
              "text                                                         \n",
              "Sorry, I'll call later                               30   ham\n",
              "I cant pick the phone right now. Pls send a mes...   12   ham\n",
              "Ok...                                                10   ham\n",
              "Your opinion about me? 1. Over 2. Jada 3. Kusru...    4   ham\n",
              "Wen ur lovable bcums angry wid u, dnt take it s...    4   ham\n",
              "Please call our customer service representative...    4  spam\n",
              "Okie                                                  4   ham\n",
              "Say this slowly.? GOD,I LOVE YOU &amp; I NEED Y...    4   ham\n",
              "7 wonders in My WORLD 7th You 6th Ur style 5th ...    4   ham\n",
              "Ok.                                                   4   ham"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFeae5DPnnrn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "632f0e9e-7f30-4d46-8646-7d147b9b6dfc"
      },
      "source": [
        "spam_messages = messages[messages[\"category\"] == \"spam\"][\"text\"]\n",
        "ham_messages = messages[messages[\"category\"] == \"ham\"][\"text\"]\n",
        "\n",
        "spam_words = []\n",
        "ham_words = []\n",
        "\n",
        "# Since this is just classifying the message as spam or ham, we can use isalpha(). \n",
        "# This will also remove the not word in something like can't etc. \n",
        "# In a sentiment analysis setting, its better to use \n",
        "# sentence.translate(string.maketrans(\"\", \"\", ), chars_to_remove)\n",
        "\n",
        "def extractSpamWords(spamMessages):\n",
        "    global spam_words\n",
        "    words = [word.lower() for word in word_tokenize(spamMessages) if word.lower() not in stopwords.words(\"english\") and word.lower().isalpha()]\n",
        "    spam_words = spam_words + words\n",
        "    \n",
        "def extractHamWords(hamMessages):\n",
        "    global ham_words\n",
        "    words = [word.lower() for word in word_tokenize(hamMessages) if word.lower() not in stopwords.words(\"english\") and word.lower().isalpha()]\n",
        "    ham_words = ham_words + words\n",
        "\n",
        "spam_messages.apply(extractSpamWords)\n",
        "ham_messages.apply(extractHamWords)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       None\n",
              "1       None\n",
              "3       None\n",
              "4       None\n",
              "6       None\n",
              "        ... \n",
              "5565    None\n",
              "5568    None\n",
              "5569    None\n",
              "5570    None\n",
              "5571    None\n",
              "Name: text, Length: 4825, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T53FeheTn-Km",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "7668443e-f661-42bb-86d7-ef73b37afadb"
      },
      "source": [
        "spam_words = np.array(spam_words)\n",
        "print(\"Top 10 Spam words are :\\n\")\n",
        "pd.Series(spam_words).value_counts().head(n = 10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 Spam words are :\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "call      346\n",
              "free      217\n",
              "txt       156\n",
              "u         144\n",
              "ur        144\n",
              "mobile    123\n",
              "text      121\n",
              "stop      114\n",
              "claim     113\n",
              "reply     104\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c92juvq1oP_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "17e70bf8-7545-457a-af9f-9800df310233"
      },
      "source": [
        "ham_words = np.array(ham_words)\n",
        "print(\"Top 10 Ham words are :\\n\")\n",
        "pd.Series(ham_words).value_counts().head(n = 10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 Ham words are :\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "u       974\n",
              "gt      318\n",
              "lt      316\n",
              "get     301\n",
              "go      246\n",
              "ok      246\n",
              "got     242\n",
              "ur      237\n",
              "know    234\n",
              "like    231\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4h5-g0DoTk_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "0131789e-bf67-4efd-c558-558f06728d4b"
      },
      "source": [
        "messages[\"messageLength\"] = messages[\"text\"].apply(len)\n",
        "messages[\"messageLength\"].describe()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    5572.000000\n",
              "mean       80.118808\n",
              "std        59.690841\n",
              "min         2.000000\n",
              "25%        36.000000\n",
              "50%        61.000000\n",
              "75%       121.000000\n",
              "max       910.000000\n",
              "Name: messageLength, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us1jefykovc9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "9187ac6a-70ae-4913-b7d8-baf43ce90ac6"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def cleanText(message):\n",
        "    \n",
        "    message = message.translate(str.maketrans('', '', string.punctuation))\n",
        "    words = [stemmer.stem(word) for word in message.split() if word.lower() not in stopwords.words(\"english\")]\n",
        "    \n",
        "    return \" \".join(words)\n",
        "\n",
        "messages[\"text\"] = messages[\"text\"].apply(cleanText)\n",
        "messages.head(n = 10)    "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "      <th>messageLength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>go jurong point crazi avail bugi n great world...</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>ok lar joke wif u oni</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>free entri 2 wkli comp win fa cup final tkts 2...</td>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>u dun say earli hor u c alreadi say</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>nah dont think goe usf live around though</td>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>spam</td>\n",
              "      <td>freemsg hey darl 3 week word back id like fun ...</td>\n",
              "      <td>148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ham</td>\n",
              "      <td>even brother like speak treat like aid patent</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ham</td>\n",
              "      <td>per request mell mell oru minnaminungint nurun...</td>\n",
              "      <td>160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>spam</td>\n",
              "      <td>winner valu network custom select receivea å£9...</td>\n",
              "      <td>158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>spam</td>\n",
              "      <td>mobil 11 month u r entitl updat latest colour ...</td>\n",
              "      <td>154</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  category                                               text  messageLength\n",
              "0      ham  go jurong point crazi avail bugi n great world...            111\n",
              "1      ham                              ok lar joke wif u oni             29\n",
              "2     spam  free entri 2 wkli comp win fa cup final tkts 2...            155\n",
              "3      ham                u dun say earli hor u c alreadi say             49\n",
              "4      ham          nah dont think goe usf live around though             61\n",
              "5     spam  freemsg hey darl 3 week word back id like fun ...            148\n",
              "6      ham      even brother like speak treat like aid patent             77\n",
              "7      ham  per request mell mell oru minnaminungint nurun...            160\n",
              "8     spam  winner valu network custom select receivea å£9...            158\n",
              "9     spam  mobil 11 month u r entitl updat latest colour ...            154"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0hI2LKzo84n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3e0e4f5-4a77-4ce7-e03e-098a88c9d833"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vec = TfidfVectorizer(encoding = \"latin-1\", strip_accents = \"unicode\", stop_words = \"english\")\n",
        "features = vec.fit_transform(messages[\"text\"])\n",
        "print(features.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5572, 7903)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1Uq67NpqLXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encodeCategory(cat):\n",
        "    if cat == \"spam\":\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "        \n",
        "messages[\"category\"] = messages[\"category\"].apply(encodeCategory)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, messages[\"category\"], stratify = messages[\"category\"], test_size = 0.2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83aXYDhuqSIT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "58f4e177-1f19-48b6-e648-5bd7248793bd"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "gaussianNb = MultinomialNB()\n",
        "gaussianNb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = gaussianNb.predict(X_test)\n",
        "c=gaussianNb.predict_proba(X_test)\n",
        "print(fbeta_score(y_test, y_pred, beta = 0.5))\n",
        "print(\"Accuracy value for LR\",roc_auc_score(y_test,y_pred))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9203539823008849\n",
            "Accuracy value for LR 0.848993288590604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sslT4AoJqhlZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a1281dca-9644-40f0-8661-8de405826b6a"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "classifier3=SVC(probability=True)\n",
        "from sklearn.metrics import roc_auc_score,accuracy_score\n",
        "classifier=LogisticRegression(random_state=0)\n",
        "classifier.fit(X_train,y_train)\n",
        "ypred=classifier.predict(X_test)\n",
        "print(\"Accuracy score of LR\",accuracy_score(y_test,ypred))\n",
        "y_prob=classifier.predict_proba(X_test)\n",
        "df=pd.DataFrame()\n",
        "df['pred']=ypred\n",
        "survived=pd.get_dummies(df.pred,prefix='Survived')\n",
        "df=pd.concat([df,survived],axis=1)\n",
        "df.drop('pred',inplace=True,axis=1)\n",
        "df2=pd.DataFrame()\n",
        "classifier3.fit(X_train,y_train)\n",
        "ypred3=classifier3.predict(X_test)\n",
        "print(\"Accuracy score of SVM\",accuracy_score(y_test,ypred3))\n",
        "'''df2['pred']=ypred3'''\n",
        "y_prob3=classifier3.predict_proba(X_test)\n",
        "'''survived2=pd.get_dummies(df2.pred,prefix='Surviveds')\n",
        "df=pd.concat([df,survived2],axis=1)'''\n",
        "df_kmeans=pd.DataFrame()\n",
        "df_kmeans['Survived_k']=y_pred\n",
        "survivedk=pd.get_dummies(df_kmeans.Survived_k,prefix='Survivedk')\n",
        "df['Survivedk_0']=survivedk['Survivedk_0']\n",
        "df['Survivedk_1']=survivedk['Survivedk_1']\n",
        "print(\"Accuracy score of SVM\",roc_auc_score(y_test,y_pred))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score of LR 0.9497757847533632\n",
            "Accuracy score of SVM 0.8663677130044843\n",
            "Accuracy score of SVM 0.848993288590604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BYXnXDiqlSg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "929fa499-9e7d-4a7c-cc7a-0a078035399a"
      },
      "source": [
        "print(\"Accuracy value for LR\",accuracy_score(y_test,ypred))\n",
        "print(\"AUC for for LR\",roc_auc_score(y_test,ypred))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy value for LR 0.9497757847533632\n",
            "AUC for for LR 0.8120805369127517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz9iyb9iv_i0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXCw8kQObWVU",
        "colab_type": "text"
      },
      "source": [
        "**Generating normalized matrices Km and Kc using the method mentioned in paper 2.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CepLLTmuv3FV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m=np.array(df)\n",
        "s=np.zeros(shape=(m.shape[0],m.shape[0]))\n",
        "for i in range(m.shape[0]):\n",
        "  for j in range(m.shape[0]):\n",
        "    if i!=j:\n",
        "      sum1=0\n",
        "      for k in range(2,m.shape[1]):\n",
        "        sum1=sum1+(m[i][k]*m[j][k])\n",
        "      s[i][j]=sum1\n",
        "w=s\n",
        "d=np.zeros(shape=w.shape[0])\n",
        "for i in range(d.shape[0]):\n",
        "  d[i]=np.sum(w[i],axis=0)\n",
        "wnew=w/d.reshape(d.shape[0],1)\n",
        "Km=wnew     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir_FOeVcboVq",
        "colab_type": "text"
      },
      "source": [
        "**Checking whether the matrices have been normalized or not i.e making them bistochastic **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck8_a_D12H0d",
        "colab_type": "code",
        "outputId": "66b09deb-1358-4c7e-8e08-48bcffdabf34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(np.sum(Km,axis=1))\n",
        "print(np.sum(Km,axis=0))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 1. ... 1. 1. 1.]\n",
            "[1. 1. 1. ... 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0ZiuK9nb9Cw",
        "colab_type": "text"
      },
      "source": [
        "Similarly for Kc we generate a bistochastic matrix where m=the co-occurence matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHn5XuG72Ijh",
        "colab_type": "code",
        "outputId": "0c45453c-be61-4369-dfc1-d8192e35d498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "m=np.array(df)\n",
        "s=np.zeros(shape=(m.shape[0],m.shape[0]))\n",
        "for i in range(m.shape[0]):\n",
        "  for j in range(m.shape[0]):\n",
        "    if i!=j:\n",
        "      sum1=0\n",
        "      for k in range(2,m.shape[1]):\n",
        "        sum1=sum1+(m[i][k]*m[j][k])\n",
        "      s[i][j]=sum1\n",
        "m=s   \n",
        "s=np.zeros(shape=(m.shape[0],m.shape[0]))\n",
        "for i in range(m.shape[0]):\n",
        "  for j in range(m.shape[0]):\n",
        "    if i!=j:\n",
        "      sum1=0\n",
        "      for k in range(2,m.shape[1]):\n",
        "        sum1=sum1+(m[i][k]*m[j][k])\n",
        "      s[i][j]=sum1   \n",
        "d=np.zeros(shape=w.shape[0])\n",
        "w=s\n",
        "for i in range(d.shape[0]):\n",
        "  d[i]=np.sum(w[i],axis=0)\n",
        "wnew=w/d.reshape(d.shape[0],1)  \n",
        "Kc=wnew\n",
        "Kc"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a455f7803cf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0msum1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0msum1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAFnKTDscdnX",
        "colab_type": "text"
      },
      "source": [
        "**Same, checking for bi-stochastic behaviour**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlvTfnrF2Smi",
        "colab_type": "code",
        "outputId": "e117587f-579a-4e86-c6fc-0ab62a99f517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "  np.sum(Kc,axis=1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., ..., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmJ1FML0ckLi",
        "colab_type": "text"
      },
      "source": [
        "**Generating Fg and Fo i'e the Group class matrix and Object Class matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTO5d9Ay2Xc5",
        "colab_type": "code",
        "outputId": "c64816c2-2a7c-4ad7-afe3-6041ad130336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "\n",
        "objclass_mat1=pd.DataFrame(y_prob,columns=['Survived','NotSurvived'])\n",
        "objclass_mat2=pd.DataFrame(y_prob3,columns=['Survived','NotSurvived'])\n",
        "df_t=pd.DataFrame(objclass_mat1)\n",
        "Fo=np.array(df_t)\n",
        "gpclass_mat=pd.DataFrame(columns=['Survived','NotSurvived'],index=['class1surv','class1notsurv','class2surv','class2notsurv'])\n",
        "prob1s=np.sum(objclass_mat1.Survived)/len(objclass_mat1)\n",
        "prob1ns=np.sum(objclass_mat1.NotSurvived)/len(objclass_mat1)\n",
        "prob2s=np.sum(objclass_mat2.Survived)/len(objclass_mat2)\n",
        "prob2ns=np.sum(objclass_mat2.NotSurvived)/len(objclass_mat2)\n",
        "list1=[]\n",
        "list1.append(prob1s)\n",
        "list1.append(prob1ns)\n",
        "list1.append(prob2s)\n",
        "list1.append(prob2ns)\n",
        "\n",
        "list2=[]\n",
        "list2.append(prob1ns)\n",
        "list2.append(prob1s)\n",
        "list2.append(prob2ns)\n",
        "list2.append(prob2s)\n",
        "gpclass_mat['Survived']=list1\n",
        "gpclass_mat['NotSurvived']=list2\n",
        "gpclass_mat\n",
        "Fg=np.array(gpclass_mat)\n",
        "Fg"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.86961239, 0.13038761],\n",
              "       [0.13038761, 0.86961239],\n",
              "       [0.8663114 , 0.1336886 ],\n",
              "       [0.1336886 , 0.8663114 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRxO_k8tct-4",
        "colab_type": "text"
      },
      "source": [
        "**Similarly, generating  Yo and Yg**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovw7WJ7S2pQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n=2\n",
        "C=survived.shape[1]\n",
        "G=survived.shape[1]*n\n",
        "\n",
        "Yo=np.zeros(shape=(df.shape[0],C))\n",
        "cal_y=np.array(df)\n",
        "for i in range(Yo.shape[0]):\n",
        "  Yo[i][0]=(cal_y[i][0]+cal_y[i][2])/C\n",
        "for i in range(Yo.shape[0]):\n",
        "  Yo[i][1]=(cal_y[i][1]+cal_y[i][3])/C"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SR3KDg3q87U",
        "colab_type": "code",
        "outputId": "ca4c2c00-8193-4501-c3cb-837844cf3b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "G"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYZkiSfs2w2I",
        "colab_type": "code",
        "outputId": "fb0f4249-0b7a-4032-df20-9616a2010af0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "m=np.array(df)\n",
        "Yg=np.zeros(shape=(m.shape[1],2))\n",
        "c=np.sum(m,axis=0)\n",
        "for i in range(Yg.shape[0]):\n",
        "  if i%2==0:\n",
        "    Yg[i][0]=c[i]/m.shape[0]\n",
        "    Yg[i][1]=1-Yg[i][0]\n",
        "  else:\n",
        "    Yg[i][1]=c[i]/m.shape[0]\n",
        "    Yg[i][0]=1-Yg[i][1]  \n",
        "Yg"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.91659193, 0.08340807],\n",
              "       [0.91659193, 0.08340807],\n",
              "       [0.90672646, 0.09327354],\n",
              "       [0.90672646, 0.09327354]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTa6_2DsdIlO",
        "colab_type": "text"
      },
      "source": [
        "Genearting "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hcY4DpgduI4",
        "colab_type": "text"
      },
      "source": [
        "Checking for the conditions specified in eq 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHAljZHU21jH",
        "colab_type": "code",
        "outputId": "7f81ce84-8c8c-4125-b8a5-6346014f758e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "Fo_cond=True\n",
        "Fg_cond=True\n",
        "for i in range(Fo.shape[0]):\n",
        "  if np.linalg.norm((Fo[i]), ord=1)!=1:\n",
        "    Fo_cond=False\n",
        "for i in range(Fg.shape[0]):\n",
        "  if np.linalg.norm((Fg[i]), ord=1)!=1:\n",
        "    Fo_cond=False    \n",
        "print(Fo_cond)\n",
        "print(Fg_cond)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7YlgtdbeCFc",
        "colab_type": "text"
      },
      "source": [
        "**Generating diagonal matrices Dm and Dc**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8cG1naJ3Cbk",
        "colab_type": "code",
        "outputId": "73cb08f1-c3b4-4342-a78c-01a7b36bcb3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_dm=np.zeros(shape=(Km.shape))\n",
        "test_Km=Km\n",
        "for i in range(test_Km.shape[0]):\n",
        "  test_dm[i]=np.sum(test_Km[i],axis=0)\n",
        "Dm=np.diag(np.diag(test_dm))\n",
        "print(\"Shape of DM\",Dm.shape)\n",
        "test_dc=np.zeros(shape=(Kc.shape))\n",
        "test_Kc=Kc\n",
        "for i in range(test_Kc.shape[0]):\n",
        "  test_dc[i]=np.sum(test_Kc[i],axis=0)\n",
        "Dc=np.diag(np.diag(test_dc))\n",
        "print(\"shape of Dc\",Dc.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of DM (1115, 1115)\n",
            "shape of Dc (1115, 1115)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv1sYezy3Hwa",
        "colab_type": "code",
        "outputId": "89e37970-bccc-4dd9-ce26-19e081c2b983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(\"KHM\",Km.shape)\n",
        "print(\"KHC\",Kc.shape)\n",
        "print(\"Fo\",Fo.shape)\n",
        "print(\"Fg\",Fg.shape)\n",
        "print(\"Yo\",Yo.shape)\n",
        "print(\"Yg\",Yg.shape)\n",
        "print(\"Dm\",Dm.shape)\n",
        "print(\"Dc\",Dc.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KHM (1115, 1115)\n",
            "KHC (1115, 1115)\n",
            "Fo (1115, 2)\n",
            "Fg (4, 2)\n",
            "Yo (1115, 2)\n",
            "Yg (4, 2)\n",
            "Dm (1115, 1115)\n",
            "Dc (1115, 1115)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCBI0yrwqzOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5InaYBP3Lfq",
        "colab_type": "code",
        "outputId": "d54ab75d-5af9-49b2-9182-58f69872ed06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Yg=np.pad(Yg,pad_width=[(0, Km.shape[0]-G), (0, 0)])\n",
        "Yg.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1115, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro1G66qS5Pzv",
        "colab_type": "code",
        "outputId": "5eb1bb9b-fb98-43d7-9fbc-e5da02a3230f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(\"KHM\",Km.shape)\n",
        "print(\"KHC\",Kc.shape)\n",
        "print(\"Fo\",Fo.shape)\n",
        "print(\"Fg\",Fg.shape)\n",
        "print(\"Yo\",Yo.shape)\n",
        "print(\"Yg\",Yg.shape)\n",
        "print(\"Dm\",Dm.shape)\n",
        "print(\"Dc\",Dc.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KHM (1115, 1115)\n",
            "KHC (1115, 1115)\n",
            "Fo (1115, 2)\n",
            "Fg (4, 2)\n",
            "Yo (1115, 2)\n",
            "Yg (1115, 2)\n",
            "Dm (1115, 1115)\n",
            "Dc (1115, 1115)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OVQOQf35Tb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "threshold=sys.float_info.epsilon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTILUgDfeX91",
        "colab_type": "text"
      },
      "source": [
        "**Implementing algorithm 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaOULbFt5XBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha=0.40\n",
        "beta=0.35\n",
        "gamma=0.45\n",
        "delta=0.5\n",
        "Fo_t=np.zeros(shape=Fo.shape)\n",
        "Fg_t=np.zeros(shape=Fg.shape)\n",
        "Fot_old=np.zeros(shape=Fo.shape)\n",
        "Fo_t=Fo\n",
        "Fg_t=Fg\n",
        "while LA.norm((Fo_t-Fot_old),'fro')>threshold:\n",
        "  Fot_old=Fo_t\n",
        "  Fg_t=np.matmul((np.linalg.inv(2*delta*np.ones(shape=Dm.shape)+(alpha*Dm))),((alpha*np.matmul(Km,Fo_t))+(2*delta*Yg)))\n",
        "  Fo_t=np.matmul((np.linalg.inv((alpha*Dm)+(2*beta*Dc)-(beta*np.matmul(np.identity(Kc.shape[0]),Kc))-(beta*np.matmul(np.ones(shape=Kc.shape),Kc))+(2*gamma*np.identity(Kc.shape[0])))),((alpha*np.matmul(Km,Fg_t))+(2*gamma*Yo)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhH4nVo_5e6_",
        "colab_type": "code",
        "outputId": "79e5d49b-0864-44bd-d1e1-8b5f91d8bee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "result=np.zeros(shape=Fo_t.shape[0])\n",
        "for i in range(Fo_t.shape[0]):\n",
        "  if Fo_t[i][0]<Fo_t[i][1]:\n",
        "    result[i]=1\n",
        "print(\"Accuracy value for Ensemble model\",accuracy_score(y_test,result))\n",
        "print(\"AUC for for Ensemble model\",roc_auc_score(y_test,result))\n",
        "print(\"Accuracy value for LR\",accuracy_score(y_test,ypred))\n",
        "print(\"AUC for for LR\",roc_auc_score(y_test,ypred))\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy value for Ensemble model 0.9668161434977578\n",
            "AUC for for Ensemble model 0.8758389261744967\n",
            "Accuracy value for LR 0.9497757847533632\n",
            "AUC for for LR 0.8120805369127517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUskNBtfe5C7",
        "colab_type": "text"
      },
      "source": [
        "**We see that the ensemble model performs better than the Random forest, Kneigbours and Svm models and is at par with the Logistic regression model that gives us the best accuracy. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PPCRZWO5qq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}